{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '/home/Talen/foragefish_classifier/configs/exp_resnet18.yaml'\n",
    "split = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/Talen/foragefish_classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from train import create_dataloader, load_model       # NOTE: since we're using these functions across files, it could make sense to put them in e.g. a \"util.py\" script.\n",
    "\n",
    "# load config\n",
    "print(f'Using config \"{config}\"')\n",
    "cfg = yaml.safe_load(open(config, 'r'))\n",
    "\n",
    "\n",
    "# setup entities\n",
    "dl_test = create_dataloader(cfg, split='test')\n",
    "\n",
    "# load model\n",
    "model, epoch = load_model(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "This is up to you to figure out now. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device) # puts model weights on to gpu\n",
    "model.eval() # changes model to eval / inference mode\n",
    "\n",
    "progressBar = trange(len(dl_test))\n",
    "pred_all = []\n",
    "argmax_all = []\n",
    "img_list = []\n",
    "for idx, (data, labels) in enumerate(dl_test):       # see the last line of file \"dataset.py\" where we return the image tensor (data) and label\n",
    "\n",
    "    # put data and labels on device\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    prediction = model(data) \n",
    "    # visualize image that's stored in a batch in variable 'data' (this will be a for loop that iterates a batch)\n",
    "    # use argmax() over the prediction in a single image, apply it to every image's corresponding prediction.\n",
    "    #In a list, store labels. ANd then compare the g-t classes and prediction. Create a pandas dataframe with three columns: image_name, gt, and predictions. \n",
    "    \n",
    "    # Now we use argmax() over the prediction pair of numbers, and apply it to every image's corresponding prediction.\n",
    "    argmax = prediction.argmax(dim=1)\n",
    "\n",
    "    print(argmax)\n",
    "    \n",
    "    # print(argmax)\n",
    "    argmax_all.extend(argmax.detach().cpu().numpy())\n",
    "    \n",
    "    \n",
    "\n",
    "    # store the prediction in a list\n",
    "    # pred_all.append(prediction.detach().cpu().numpy()[0])\n",
    "    pred_all.append(prediction.detach().cpu().numpy())\n",
    "    img_list.extend(data)\n",
    "    \n",
    "\n",
    "\n",
    "# step 1 -visualize predictions + ground truth in matplotlib\n",
    "# Step 2 - look up weights + biases, how to set them up in the model to log during training\n",
    "# Step 3 - set up experiments so that when I start a new training run, it generates an experimental folder with the right name \n",
    "# copy config file to each experiment folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list\n",
    "\n",
    "\n",
    "# Now we use argmax() over the prediction pair of numbers, and apply it to every image's corresponding prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make argmax into a list (same as pred_all) and print it:\n",
    "print(argmax_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Now we make a list of our ground-truth labels:\n",
    "# gt_all = []\n",
    "# for idx, (data, labels) in enumerate(dl_test):\n",
    "#      gt_all.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "# print(gt_all)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make a pandas dataframe with three columns: image_name, gt, and predictions.\n",
    "import pandas as pd\n",
    "# df = pd.DataFrame({'gt': gt_all, 'pred':argmax_all, 'raw_pred': pred_all})\n",
    "df = pd.DataFrame({'gt': gt_all, 'pred':argmax_all})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we visualize the predictions and ground truth in matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Let's visualize the first 10 images\n",
    "# for i in range(10):\n",
    "#     plt.imshow(data[i].permute(1,2,0))\n",
    "#     plt.title(f'Ground truth: {gt_all[i]}, Prediction: {argmax_all[i]}')\n",
    "#     plt.show()\n",
    "\n",
    "# Now lets visualize all images in the dataset, with their ground truth and predictions:\n",
    "# for i in range(len(df)):\n",
    "#     plt.imshow(data[i].permute(1,2,0))\n",
    "#     plt.title(f'Ground truth: {gt_all[i]}, Prediction: {argmax_all[i]}')\n",
    "#     plt.show()\n",
    "\n",
    "# This visualizes one batch of images, but we want to visualize all images in the dataset.\n",
    "for i in range(len(df)):\n",
    "    # Now we visualize the images in a batch of 4 columns and 4 rows:\n",
    "    plt.imshow(img_list[i].cpu().permute(1,2,0))\n",
    "    plt.title(f'Ground truth: {gt_all[i]}, Prediction: {argmax_all[i]}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing now but in a smaller grid\n",
    "\n",
    "def display_batch(img_list, gt_all, argmax_all, start_idx=0):\n",
    "    # Create 4x3 grid with adjusted figure size\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Display up to 12 images per batch\n",
    "    for i in range(12):\n",
    "        idx = start_idx + i\n",
    "        if idx >= len(img_list):\n",
    "            break\n",
    "            \n",
    "        # Display image and labels\n",
    "        axes[i].imshow(img_list[idx].cpu().permute(1,2,0))\n",
    "        axes[i].set_title(f'Image {idx}\\nGT: {gt_all[idx]}\\nPred: {argmax_all[idx]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display all images in batches of 12\n",
    "for batch_start in range(0, len(df), 12):\n",
    "    display_batch(img_list, gt_all, argmax_all, batch_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps: visualize and evaluate\n",
    "# Visualize - look @ images and compare to prediction we got.\n",
    "\n",
    "# Now we want to plot a histogram visualizing the distribution of the confidence in each image by our model, seperated by empty and forage fish images:\n",
    "import seaborn as sns\n",
    "sns.histplot(df['gt'], color='blue', alpha=0.5, label='Ground Truth')\n",
    "sns.histplot(df['pred'], color='red', alpha=0.5, label='Prediction')\n",
    "plt.legend()\n",
    "# first we need to conda install seaborn:\n",
    "# conda install seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.histplot(df['gt'], color='blue', alpha=0.5, label='Ground Truth')\n",
    "sns.histplot(df['pred'], color='red', alpha=0.5, label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# These should have different colours with an alpha value to see the overlap.\n",
    "# We will use the matplotlib library to plot this.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Now we want to visualize the predictions and ground truth in matplotlib in a plot:\n",
    "# plt.plot(df['gt'], label='Ground Truth')\n",
    "# plt.plot(df['pred'], label='Prediction')\n",
    "# #give the x-axis a label\n",
    "# plt.xlabel('Image')\n",
    "# #give the y-axis a label\n",
    "# plt.ylabel('Class')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# import sklearn\n",
    "# print(sklearn.__version__)\n",
    "\n",
    "\n",
    "# # Now print the accuracy of the model:\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy = accuracy_score(df['gt'], df['pred'])\n",
    "# print(f'Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv4ecology2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
