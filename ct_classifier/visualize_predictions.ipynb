{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '/home/Talen/foragefish_classifier/configs/exp_resnet18.yaml'\n",
    "split = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/Talen/foragefish_classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from train import create_dataloader, load_model       # NOTE: since we're using these functions across files, it could make sense to put them in e.g. a \"util.py\" script.\n",
    "\n",
    "# load config\n",
    "print(f'Using config \"{config}\"')\n",
    "cfg = yaml.safe_load(open(config, 'r'))\n",
    "\n",
    "\n",
    "# setup entities\n",
    "dl_test = create_dataloader(cfg, split='test')\n",
    "\n",
    "\n",
    "# create model\n",
    "\n",
    "from model import CustomResNet18\n",
    "import glob\n",
    "\n",
    "'''\n",
    "    Creates a model instance and loads the latest model state weights.\n",
    "'''\n",
    "model = CustomResNet18(cfg['num_classes'])         # create an object instance of our CustomResNet18 class\n",
    "\n",
    "# load latest model state\n",
    "\n",
    "checkpoint = torch.load(\"/home/Talen/foragefish_classifier/model_states_test/best.pt\")\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "This is up to you to figure out now. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup entities\n",
    "dl_test = create_dataloader(cfg, split='test')\n",
    "dataset = dl_test.dataset\n",
    "# filenames = [entry[0] for entry in dataset.data]\n",
    "# print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device) # puts model weights on to gpu\n",
    "model.eval() # changes model to eval / inference mode\n",
    "\n",
    "\n",
    "progressBar = trange(len(dl_test))\n",
    "pred_all = []\n",
    "argmax_all = []\n",
    "# img_list = []\n",
    "confs_list = []\n",
    "filename_list = []\n",
    "gt_all = []\n",
    "for idx, (data, labels, image_names) in enumerate(dl_test):       # see the last line of file \"dataset.py\" where we return the image tensor (data) and label\n",
    "\n",
    "    # put data and labels on device\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    prediction = model(data) \n",
    "   \n",
    "    # visualize image that's stored in a batch in variable 'data' (this will be a for loop that iterates a batch)\n",
    "    # use argmax() over the prediction in a single image, apply it to every image's corresponding prediction.\n",
    "    \n",
    "    # Now we use argmax() over the prediction pair of numbers, and apply it to every image's corresponding prediction.\n",
    "    argmax = prediction.argmax(dim=1)\n",
    "\n",
    "    # # print(argmax)\n",
    "    \n",
    "    # # print(argmax)\n",
    "    argmax_all.extend(argmax.detach().cpu().numpy())\n",
    "\n",
    "    # Using softmax to get probabilities\n",
    "    probabilities = F.softmax(prediction, dim=1)\n",
    "\n",
    "      # get the predicted labels and their confidence scores\n",
    "    pred_label = torch.argmax(probabilities, dim=1)\n",
    "    confidence_scores = torch.max(probabilities, dim=1).values\n",
    "    confidence_scores = confidence_scores.tolist()\n",
    "    # print(confidence_scores)\n",
    "\n",
    "    \n",
    "\n",
    "    # store the prediction in a list\n",
    "    # pred_all.append(prediction.detach().cpu().numpy()[0])\n",
    "    pred_all.extend(prediction.detach().cpu().numpy())\n",
    "    # img_list.extend(data)\n",
    "    confs_list.extend(confidence_scores)\n",
    "    filename_list.extend(image_names)\n",
    "    gt_all.extend(labels.detach().cpu().numpy())\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# step 1 -visualize predictions + ground truth in matplotlib\n",
    "# Step 2 - look up weights + biases, how to set them up in the model to log during training\n",
    "# Step 3 - set up experiments so that when I start a new training run, it generates an experimental folder with the right name \n",
    "# copy config file to each experiment folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred_all)\n",
    "#now we make pred_all a numpy array\n",
    "pred_all = np.array(pred_all)\n",
    "print (pred_all.shape)\n",
    "print (len(confs_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section of code simply creates a list of confidence scores for each image in the dataset \n",
    "by calculating the difference between the highest and second highest prediction values for each image \n",
    "(there's only two values right now, but we may have more in the future).\n",
    "\"\"\"\n",
    "\n",
    "# Now we can use the info in pred_all to calculate a confidence score for each image in the dataset:\n",
    "# now we'll write the code to do this:\n",
    "# we'll calculate the confidence score for each image in the dataset.\n",
    "# confidence score = max(prediction) - second_max(prediction)\n",
    "# we'll store this in a list called confidence_scores\n",
    "# confidence_scores = []\n",
    "# for i in range(len(pred_all)):\n",
    "#     # we'll use numpy's argsort() function to get the indices of the sorted array\n",
    "#     sorted_indices = np.argsort(pred_all[i])\n",
    "#     # we'll get the two highest values from the sorted array\n",
    "#     highest = sorted_indices[-1]\n",
    "#     second_highest = sorted_indices[-2]\n",
    "#     # we'll calculate the confidence score\n",
    "#     confidence_score = pred_all[i][1] - pred_all[i][0]  # positive for class 1, negative for class 0\n",
    "#     # we'll store the confidence score in the list\n",
    "#     confidence_scores.append(confidence_score)\n",
    "\n",
    "# # now we'll print the confidence scores\n",
    "# print(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now we make a list of our ground-truth labels:\n",
    "# gt_all = []\n",
    "# for idx, (data, labels, _) in enumerate(dl_test):\n",
    "#      gt_all.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "# # print(gt_all)\n",
    "\n",
    "# # Now we check the type of gt_all:\n",
    "# type(gt_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert lists to numpy arrays for consistent handling\n",
    "# confidence_scores = np.array(confidence_scores)\n",
    "argmax_all = np.array(argmax_all)\n",
    "gt_all = np.array(gt_all)\n",
    "\n",
    "# Create DataFrame with all components\n",
    "df_combined = pd.DataFrame({\n",
    "    # 'confidence_scores': confidence_scores.tolist(),  # Convert to list for DataFrame\n",
    "    'predict_class': argmax_all,\n",
    "    'ground_truth': gt_all,\n",
    "    # 'image_id': range(len(img_list)),\n",
    "    'filenames':filename_list,\n",
    "    'confs':confs_list\n",
    "})\n",
    "\n",
    "# Set image_id as index\n",
    "# df_combined.set_index('image_id', inplace=True)\n",
    "#df_combined.set_index('filenames', inplace=True)\n",
    "\n",
    "# Verify structure\n",
    "print(\"DataFrame shape:\", df_combined.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create density plots with filled areas\n",
    "for gt_class in df_combined['ground_truth'].unique():\n",
    "    # Get confidence scores for this class\n",
    "    class_scores = df_combined[df_combined['ground_truth'] == gt_class]['confs']\n",
    "    \n",
    "    # Plot density with filled area\n",
    "    sns.kdeplot(data=class_scores, \n",
    "                fill=True,  # Fill area under curve\n",
    "                alpha=0.5,  # Transparency\n",
    "                label=f'Class {gt_class}')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Confidence Scores')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Confidence Scores by Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create figure with two panels\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Panel 1: ground_truth = 0\n",
    "data_gt0 = df_combined[df_combined['ground_truth'] == 0]\n",
    "correct_gt0 = data_gt0[data_gt0['ground_truth'] == data_gt0['predict_class']]['confs']\n",
    "incorrect_gt0 = data_gt0[data_gt0['ground_truth'] != data_gt0['predict_class']]['confs']\n",
    "\n",
    "# Panel 2: ground_truth = 1\n",
    "data_gt1 = df_combined[df_combined['ground_truth'] == 1]\n",
    "correct_gt1 = data_gt1[data_gt1['ground_truth'] == data_gt1['predict_class']]['confs']\n",
    "incorrect_gt1 = data_gt1[data_gt1['ground_truth'] != data_gt1['predict_class']]['confs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_gt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sns.kdeplot(data=correct_gt0, fill=True, alpha=0.5, label='Correct', ax=ax1)\n",
    "\n",
    "sns.displot(data=data_gt1, x = data_gt1['confs'], hue = 'predict_class', fill = True, alpha = 0.5)\n",
    "# sns.displot(data=incorrect_gt1, label='InCorrect')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sns.kdeplot(data=incorrect_gt0, fill=True, alpha=0.5, label='Incorrect', ax=ax1)\n",
    "ax1.set_title('Ground Truth = 0 (Empty)', fontsize=16)\n",
    "ax1.set_xlabel('Confidence Scores')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "sns.kdeplot(data=correct_gt1, fill=True, alpha=0.5, label='Correct', ax=ax2)\n",
    "sns.kdeplot(data=incorrect_gt1, fill=True, alpha=0.5, label='Incorrect', ax=ax2)\n",
    "ax2.set_title('Ground Truth = 1 (Forage Fish)', fontsize=16)\n",
    "ax2.set_xlabel('Confidence Scores')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this plot, we will visualize using a histogram instead of density:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# read in results\n",
    "df = df_combined\n",
    "\n",
    "# Calculate logits\n",
    "df[\"logits\"] = np.log(df[\"confs\"] / (1 - df[\"confs\"]))\n",
    "\n",
    "# Classify as positive or negative\n",
    "df[\"label\"] = np.where(df[\"predict_class\"] == df[\"ground_truth\"], \"Positive\", \"Negative\")\n",
    "\n",
    "# choose class\n",
    "chosen_class = 1\n",
    "filtered_df = df[df[\"ground_truth\"] == chosen_class]\n",
    "\n",
    "# Plot histogram for the first class\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=filtered_df, x=\"logits\", hue=\"label\", kde=True, bins=10, alpha = 0.5)\n",
    "plt.title(f\"Logit Distribution for Class {chosen_class}\")\n",
    "plt.xlabel(\"Logits\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram for the first class\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=filtered_df, x=\"confs\", hue=\"label\", kde=True, bins=10, alpha = 0.5)\n",
    "plt.title(f\"Confidence Distribution for Class {chosen_class}\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined)\n",
    "\n",
    "\n",
    "# Now we visualize the predictions and ground truth in matplotlib\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# Let's visualize the first 10 images\n",
    "# for i in range(10):\n",
    "#     plt.imshow(data[i].permute(1,2,0))\n",
    "#     plt.title(f'Ground truth: {gt_all[i]}, Prediction: {argmax_all[i]}')\n",
    "#     plt.show()\n",
    "\n",
    "# Now lets visualize all images in the dataset, with their ground truth and predictions:\n",
    "# for i in range(len(df)):\n",
    "#     plt.imshow(data[i].permute(1,2,0))\n",
    "#     plt.title(f'Ground truth: {gt_all[i]}, Prediction: {argmax_all[i]}')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# This visualizes one batch of images, but we want to visualize all images in the dataset.\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     plt.imshow(img_list[i].cpu().permute(1,2,0))\n",
    "#     plt.title(f'Ground truth: {gt_all[i]}, Prediction: {argmax_all[i]}')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for df_combined, we plot a batch of images listing the ground truth and confidence scores for each images using the 'filenames' column:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_batch(img_list, df_combined, start_idx=0):\n",
    "    # Create 4x3 grid with adjusted figure size\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Display up to 12 images per batch\n",
    "    for i in range(12):\n",
    "        idx = start_idx + i\n",
    "        if idx >= len(img_list):\n",
    "            break\n",
    "            \n",
    "        # Get data from dataframe\n",
    "        gt = df_combined.iloc[idx]['ground_truth']\n",
    "        pred = df_combined.iloc[idx]['predict_class']\n",
    "        conf = df_combined.iloc[idx]['confs']\n",
    "        fname = df_combined.iloc[idx]['filenames']\n",
    "            \n",
    "        # Display image and labels\n",
    "        axes[i].imshow(img_list[idx].cpu().permute(1,2,0))\n",
    "        axes[i].set_title(f'File: {fname}\\nGT: {gt}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "### NOTE: This function is designed to display a batch of 12 images at a time. ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def display_images(filename_list, df_combined, num_batches=1, batch_size=12, grid_layout=(4,3)):\n",
    "    \"\"\"\n",
    "    Display specified number of image batches with metadata\n",
    "    \"\"\"\n",
    "    for batch in range(num_batches):\n",
    "        # Create figure and axes grid\n",
    "        rows, cols = grid_layout\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        # Calculate start index for current batch\n",
    "        start_idx = batch * batch_size\n",
    "        \n",
    "        # Display images in current batch\n",
    "        for i in range(batch_size):\n",
    "            idx = start_idx + i\n",
    "            if idx >= len(filename_list):\n",
    "                break\n",
    "            \n",
    "            \n",
    "            # Get metadata from dataframe\n",
    "            # gt = df_combined.iloc[idx]['ground_truth']\n",
    "            # pred = df_combined.iloc[idx]['predict_class']\n",
    "            # conf = df_combined.iloc[idx]['confs']\n",
    "            # fname = df_combined.iloc[idx][1]#'filenames']\n",
    "            gt = df_combined.iloc[idx]['ground_truth']\n",
    "            pred = df_combined.iloc[idx]['predict_class']\n",
    "            print (pred)\n",
    "            \n",
    "            conf = df_combined.iloc[idx]['confs']\n",
    "            print (conf)\n",
    "            fname = df_combined.iloc[idx]['filenames']\n",
    "            print (fname)\n",
    "\n",
    "            \n",
    "            # Display image and labels\n",
    "            img = Image.open('/mnt/class_data/group4/talen/combined_imbalanced/eccv_18_all_images_sm/' + fname)\n",
    "            axes[i].imshow(img)\n",
    "            \n",
    "            #axes[i].imshow(fname.cpu().permute(1,2,0))\n",
    "            axes[i].set_title(f'File: {fname}\\nGT: {gt}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Clear unused subplots\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage:\n",
    "# Show first batch only\n",
    "data_gt1 = df_combined[df_combined['ground_truth'] == 1]\n",
    "correct_gt1 = data_gt1[data_gt1['ground_truth'] == data_gt1['predict_class']]\n",
    "incorrect_gt1 = data_gt1[data_gt1['ground_truth'] != data_gt1['predict_class']]\n",
    "\n",
    "\n",
    "display_images(incorrect_gt1[\"filenames\"], incorrect_gt1, num_batches=3)\n",
    "\n",
    "# Show first 3 batches\n",
    "# display_images(img_list, df_combined, num_batches=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def display_images(filename_list, df_combined, num_batches=1, batch_size=12, grid_layout=(4,3)):\n",
    "    \"\"\"\n",
    "    Display specified number of image batches with metadata\n",
    "    \"\"\"\n",
    "    for batch in range(num_batches):\n",
    "        # Create figure and axes grid\n",
    "        rows, cols = grid_layout\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        # Calculate start index for current batch\n",
    "        start_idx = batch * batch_size\n",
    "        \n",
    "        # Display images in current batch\n",
    "        for i in range(batch_size):\n",
    "            idx = start_idx + i\n",
    "            if idx >= len(filename_list):\n",
    "                break\n",
    "            \n",
    "            \n",
    "            # Get metadata from dataframe\n",
    "            # gt = df_combined.iloc[idx]['ground_truth']\n",
    "            # pred = df_combined.iloc[idx]['predict_class']\n",
    "            # conf = df_combined.iloc[idx]['confs']\n",
    "            # fname = df_combined.iloc[idx][1]#'filenames']\n",
    "            gt = df_combined.iloc[idx]['ground_truth']\n",
    "            pred = df_combined.iloc[idx]['predict_class']\n",
    "            print (pred)\n",
    "            \n",
    "            conf = df_combined.iloc[idx]['confs']\n",
    "            print (conf)\n",
    "            fname = df_combined.iloc[idx]['filenames']\n",
    "            print (fname)\n",
    "\n",
    "            \n",
    "            # Display image and labels\n",
    "            img = Image.open('/mnt/class_data/group4/talen/combined_imbalanced/eccv_18_all_images_sm/' + fname)\n",
    "            axes[i].imshow(img)\n",
    "            \n",
    "            #axes[i].imshow(fname.cpu().permute(1,2,0))\n",
    "            axes[i].set_title(f'File: {fname}\\nGT: {gt}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Clear unused subplots\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage:\n",
    "# Show first batch only\n",
    "data_gt0 = df_combined[df_combined['ground_truth'] == 0]\n",
    "correct_gt0 = data_gt0[data_gt0['ground_truth'] == data_gt0['predict_class']]\n",
    "incorrect_gt0 = data_gt0[data_gt0['ground_truth'] != data_gt0['predict_class']]\n",
    "\n",
    "\n",
    "display_images(incorrect_gt0[\"filenames\"], incorrect_gt0, num_batches=3)\n",
    "\n",
    "# Show first 3 batches\n",
    "# display_images(img_list, df_combined, num_batches=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 23  11:00am - this code will take the lowest confidence correct scores, and highest confidence incorrect scores\n",
    "\n",
    "def display_confidence_cases(img_list, df_combined, n_images=20, grid_layout=(5,4)):\n",
    "    \"\"\"Display lowest confidence correct class 1 and highest confidence incorrect class 1\"\"\"\n",
    "    \n",
    "    # Filter case 1: Correct class 1 predictions (lowest confidence)\n",
    "    correct_class1 = df_combined[\n",
    "        (df_combined['ground_truth'] == 1) & \n",
    "        (df_combined['predict_class'] == 1)\n",
    "    ].sort_values('confs', ascending=True).head(n_images)\n",
    "    \n",
    "    # Filter case 2: Incorrect predictions of class 1 (highest confidence)\n",
    "    incorrect_class1 = df_combined[\n",
    "        (df_combined['ground_truth'] == 1) & \n",
    "        (df_combined['predict_class'] == 0)\n",
    "    ].sort_values('confs', ascending=False).head(n_images)\n",
    "    \n",
    "    # Display both cases\n",
    "    for case, title in zip([correct_class1, incorrect_class1], \n",
    "                          ['Lowest Confidence Correct Class 1', \n",
    "                           'Highest Confidence Incorrect Class 1']):\n",
    "        \n",
    "        if len(case) == 0:\n",
    "            print(f\"No images found for {title}\")\n",
    "            continue\n",
    "            \n",
    "        rows, cols = grid_layout\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        n_display = min(len(case), len(axes))\n",
    "        \n",
    "        # Display available images\n",
    "        for i in range(n_display):\n",
    "            idx = case.index[i]\n",
    "            row = case.iloc[i]\n",
    "            \n",
    "            # Get image and metadata\n",
    "            gt = row['ground_truth']\n",
    "            pred = row['predict_class']\n",
    "            conf = row['confs']\n",
    "            fname = row[1]  # 'filenames'\n",
    "            \n",
    "            # Display image\n",
    "            axes[i].imshow(img_list[idx].cpu().permute(1,2,0))\n",
    "            axes[i].set_title(f'File: {fname}\\nGT: {gt}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Clear remaining subplots\n",
    "        for j in range(n_display, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage\n",
    "display_confidence_cases(img_list, df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confidence_cases(img_list, df_combined, n_images=20, grid_layout=(5,4)):\n",
    "    \"\"\"Display lowest confidence correct class 1 and highest confidence incorrect class 1\"\"\"\n",
    "    \n",
    "    # Filter cases\n",
    "    correct_class1 = df_combined[\n",
    "        (df_combined['ground_truth'] == 1) & \n",
    "        (df_combined['predict_class'] == 1)\n",
    "    ].sort_values('confs', ascending=True).head(n_images)\n",
    "    \n",
    "    incorrect_class1 = df_combined[\n",
    "        (df_combined['ground_truth'] == 1) & \n",
    "        (df_combined['predict_class'] == 0)\n",
    "    ].sort_values('confs', ascending=False).head(n_images)\n",
    "    \n",
    "    # Display both cases\n",
    "    for case, title in zip([correct_class1, incorrect_class1], \n",
    "                          ['Lowest Confidence Correct Class 1', \n",
    "                           'Highest Confidence Incorrect Class 1']):\n",
    "        \n",
    "        if len(case) == 0:\n",
    "            print(f\"No images found for {title}\")\n",
    "            continue\n",
    "            \n",
    "        rows, cols = grid_layout\n",
    "        # Increase figure height to accommodate title\n",
    "        fig = plt.figure(figsize=(cols*4, rows*4 + 1))\n",
    "        \n",
    "        # Add padding for title\n",
    "        plt.subplots_adjust(top=0.93)\n",
    "        \n",
    "        # Create subplot grid\n",
    "        gs = fig.add_gridspec(rows, cols)\n",
    "        axes = [fig.add_subplot(gs[i, j]) for i in range(rows) for j in range(cols)]\n",
    "        \n",
    "        n_display = min(len(case), len(axes))\n",
    "        \n",
    "        # Display available images\n",
    "        for i in range(n_display):\n",
    "            idx = case.index[i]\n",
    "            row = case.iloc[i]\n",
    "            \n",
    "            gt = row['ground_truth']\n",
    "            pred = row['predict_class']\n",
    "            conf = row['confs']\n",
    "            fname = row[1]\n",
    "            \n",
    "            axes[i].imshow(img_list[idx].cpu().permute(1,2,0))\n",
    "            axes[i].set_title(f'File: {fname}\\nGT: {gt}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Clear remaining subplots\n",
    "        for j in range(n_display, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16, y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage\n",
    "display_confidence_cases(img_list, df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv4ecology2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
