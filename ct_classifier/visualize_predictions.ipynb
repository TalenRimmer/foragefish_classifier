{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x780040127f30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '/home/Talen/foragefish_classifier/configs/exp_resnet18.yaml'\n",
    "split = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/Talen/foragefish_classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config \"/home/Talen/foragefish_classifier/configs/exp_resnet18.yaml\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Talen/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "/home/Talen/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/Talen/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_82589/606063041.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"/home/Talen/foragefish_classifier/model_states_temp/best.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from train import create_dataloader, load_model       # NOTE: since we're using these functions across files, it could make sense to put them in e.g. a \"util.py\" script.\n",
    "\n",
    "# load config\n",
    "print(f'Using config \"{config}\"')\n",
    "cfg = yaml.safe_load(open(config, 'r'))\n",
    "\n",
    "\n",
    "# setup entities\n",
    "dl_test = create_dataloader(cfg, split='train')\n",
    "\n",
    "\n",
    "# create model\n",
    "\n",
    "from model import CustomResNet18\n",
    "import glob\n",
    "\n",
    "'''\n",
    "    Creates a model instance and loads the latest model state weights.\n",
    "'''\n",
    "model = CustomResNet18(cfg['num_classes'])         # create an object instance of our CustomResNet18 class\n",
    "\n",
    "# load latest model state\n",
    "\n",
    "checkpoint = torch.load(\"/home/Talen/foragefish_classifier/model_states_temp/best.pt\")\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "This is up to you to figure out now. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup entities\n",
    "dl_test = create_dataloader(cfg, split='test')\n",
    "dataset = dl_test.dataset\n",
    "# filenames = [entry[0] for entry in dataset.data]\n",
    "# print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [03:35<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device) # puts model weights on to gpu\n",
    "model.eval() # changes model to eval / inference mode\n",
    "\n",
    "\n",
    "progressBar = trange(len(dl_test))\n",
    "pred_all = []\n",
    "argmax_all = []\n",
    "# img_list = []\n",
    "confs_list = []\n",
    "filename_list = []\n",
    "gt_all = []\n",
    "for idx, (data, labels, image_names) in enumerate(dl_test):       # see the last line of file \"dataset.py\" where we return the image tensor (data) and label\n",
    "\n",
    "    # put data and labels on device\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    prediction = model(data) \n",
    "   \n",
    "    # visualize image that's stored in a batch in variable 'data' (this will be a for loop that iterates a batch)\n",
    "    # use argmax() over the prediction in a single image, apply it to every image's corresponding prediction.\n",
    "    \n",
    "    # Now we use argmax() over the prediction pair of numbers, and apply it to every image's corresponding prediction.\n",
    "    # argmax = prediction.argmax(dim=1)\n",
    "\n",
    "    # # print(argmax)\n",
    "    \n",
    "    # # print(argmax)\n",
    "    # argmax_all.extend(argmax.detach().cpu().numpy())\n",
    "\n",
    "    # Using softmax to get probabilities\n",
    "    probabilities = F.softmax(prediction, dim=1)\n",
    "\n",
    "      # get the predicted labels and their confidence scores\n",
    "    pred_label = torch.argmax(probabilities, dim=1)\n",
    "    confidence_scores = torch.max(probabilities, dim=1).values\n",
    "    confidence_scores = confidence_scores.tolist()\n",
    "    # print(confidence_scores)\n",
    "\n",
    "    \n",
    "\n",
    "    # store the prediction in a list\n",
    "    # pred_all.append(prediction.detach().cpu().numpy()[0])\n",
    "    pred_all.extend(prediction.detach().cpu().numpy())\n",
    "    # img_list.extend(data)\n",
    "    confs_list.extend(confidence_scores)\n",
    "    filename_list.extend(image_names)\n",
    "    gt_all.extend(labels.detach().cpu().numpy())\n",
    "    \n",
    "\n",
    "\n",
    "# step 1 -visualize predictions + ground truth in matplotlib\n",
    "# Step 2 - look up weights + biases, how to set them up in the model to log during training\n",
    "# Step 3 - set up experiments so that when I start a new training run, it generates an experimental folder with the right name \n",
    "# copy config file to each experiment folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred_all)\n",
    "#now we make pred_all a numpy array\n",
    "pred_all = np.array(pred_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section of code simply creates a list of confidence scores for each image in the dataset \n",
    "by calculating the difference between the highest and second highest prediction values for each image \n",
    "(there's only two values right now, but we may have more in the future).\n",
    "\"\"\"\n",
    "\n",
    "# Now we can use the info in pred_all to calculate a confidence score for each image in the dataset:\n",
    "# now we'll write the code to do this:\n",
    "# we'll calculate the confidence score for each image in the dataset.\n",
    "# confidence score = max(prediction) - second_max(prediction)\n",
    "# we'll store this in a list called confidence_scores\n",
    "# confidence_scores = []\n",
    "# for i in range(len(pred_all)):\n",
    "#     # we'll use numpy's argsort() function to get the indices of the sorted array\n",
    "#     sorted_indices = np.argsort(pred_all[i])\n",
    "#     # we'll get the two highest values from the sorted array\n",
    "#     highest = sorted_indices[-1]\n",
    "#     second_highest = sorted_indices[-2]\n",
    "#     # we'll calculate the confidence score\n",
    "#     confidence_score = pred_all[i][1] - pred_all[i][0]  # positive for class 1, negative for class 0\n",
    "#     # we'll store the confidence score in the list\n",
    "#     confidence_scores.append(confidence_score)\n",
    "\n",
    "# # now we'll print the confidence scores\n",
    "# print(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now we make a list of our ground-truth labels:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m gt_all \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (data, labels, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dl_test):\n\u001b[1;32m      4\u001b[0m      gt_all\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(gt_all)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Now we check the type of gt_all:\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Now we make a list of our ground-truth labels:\n",
    "# gt_all = []\n",
    "# for idx, (data, labels, _) in enumerate(dl_test):\n",
    "#      gt_all.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "# # print(gt_all)\n",
    "\n",
    "# # Now we check the type of gt_all:\n",
    "# type(gt_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m gt_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(gt_all)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create DataFrame with all components\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 'confidence_scores': confidence_scores.tolist(),  # Convert to list for DataFrame\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43margmax_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mground_truth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 'image_id': range(len(img_list)),\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilenames\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mconfs_list\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Set image_id as index\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# df_combined.set_index('image_id', inplace=True)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m df_combined\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilenames\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniconda3/envs/cv4ecology2/lib/python3.9/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "# Convert lists to numpy arrays for consistent handling\n",
    "# confidence_scores = np.array(confidence_scores)\n",
    "argmax_all = np.array(argmax_all)\n",
    "gt_all = np.array(gt_all)\n",
    "\n",
    "# Create DataFrame with all components\n",
    "df_combined = pd.DataFrame({\n",
    "    # 'confidence_scores': confidence_scores.tolist(),  # Convert to list for DataFrame\n",
    "    'predict_class': argmax_all,\n",
    "    'ground_truth': gt_all,\n",
    "    # 'image_id': range(len(img_list)),\n",
    "    'filenames':np.array(filename_list),\n",
    "    'confs':confs_list\n",
    "})\n",
    "\n",
    "# Set image_id as index\n",
    "# df_combined.set_index('image_id', inplace=True)\n",
    "df_combined.set_index('filenames', inplace=True)\n",
    "\n",
    "# Verify structure\n",
    "print(\"DataFrame shape:\", df_combined.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create density plots with filled areas\n",
    "for gt_class in df_combined['ground_truth'].unique():\n",
    "    # Get confidence scores for this class\n",
    "    class_scores = df_combined[df_combined['ground_truth'] == gt_class]['confs']\n",
    "    \n",
    "    # Plot density with filled area\n",
    "    sns.kdeplot(data=class_scores, \n",
    "                fill=True,  # Fill area under curve\n",
    "                alpha=0.5,  # Transparency\n",
    "                label=f'Class {gt_class}')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Confidence Scores')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Confidence Scores by Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create figure with two panels\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Panel 1: ground_truth = 0\n",
    "data_gt0 = df_combined[df_combined['ground_truth'] == 0]\n",
    "correct_gt0 = data_gt0[data_gt0['ground_truth'] == data_gt0['predict_class']]['confs']\n",
    "incorrect_gt0 = data_gt0[data_gt0['ground_truth'] != data_gt0['predict_class']]['confs']\n",
    "\n",
    "sns.kdeplot(data=correct_gt0, fill=True, alpha=0.5, label='Correct', ax=ax1)\n",
    "sns.kdeplot(data=incorrect_gt0, fill=True, alpha=0.5, label='Incorrect', ax=ax1)\n",
    "ax1.set_title('Ground Truth = 0 (Empty)', fontsize=16)\n",
    "ax1.set_xlabel('Confidence Scores')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Panel 2: ground_truth = 1\n",
    "data_gt1 = df_combined[df_combined['ground_truth'] == 1]\n",
    "correct_gt1 = data_gt1[data_gt1['ground_truth'] == data_gt1['predict_class']]['confs']\n",
    "incorrect_gt1 = data_gt1[data_gt1['ground_truth'] != data_gt1['predict_class']]['confs']\n",
    "\n",
    "sns.kdeplot(data=correct_gt1, fill=True, alpha=0.5, label='Correct', ax=ax2)\n",
    "sns.kdeplot(data=incorrect_gt1, fill=True, alpha=0.5, label='Incorrect', ax=ax2)\n",
    "ax2.set_title('Ground Truth = 1 (Forage Fish)', fontsize=16)\n",
    "ax2.set_xlabel('Confidence Scores')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this plot, we will visualize using a histogram instead of density:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# read in results\n",
    "df = df_combined\n",
    "\n",
    "# Calculate logits\n",
    "df[\"logits\"] = np.log(df[\"confs\"] / (1 - df[\"confs\"]))\n",
    "\n",
    "# Classify as positive or negative\n",
    "df[\"label\"] = np.where(df[\"predict_class\"] == df[\"ground_truth\"], \"Positive\", \"Negative\")\n",
    "\n",
    "# choose class\n",
    "chosen_class = 1\n",
    "filtered_df = df[df[\"ground_truth\"] == chosen_class]\n",
    "\n",
    "# Plot histogram for the first class\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=filtered_df, x=\"logits\", hue=\"label\", kde=True, bins=10, alpha = 0.5)\n",
    "plt.title(f\"Logit Distribution for Class {chosen_class}\")\n",
    "plt.xlabel(\"Logits\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram for the first class\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=filtered_df, x=\"confs\", hue=\"label\", kde=True, bins=10, alpha = 0.5)\n",
    "plt.title(f\"Confidence Distribution for Class {chosen_class}\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined)\n",
    "\n",
    "\n",
    "# Now we visualize the predictions and ground truth in matplotlib\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# Let's visualize the first 10 images\n",
    "# for i in range(10):\n",
    "#     plt.imshow(data[i].permute(1,2,0))\n",
    "#     plt.title(f'Ground truth: {gt_all[i]}, Prediction: {argmax_all[i]}')\n",
    "#     plt.show()\n",
    "\n",
    "# Now lets visualize all images in the dataset, with their ground truth and predictions:\n",
    "# for i in range(len(df)):\n",
    "#     plt.imshow(data[i].permute(1,2,0))\n",
    "#     plt.title(f'Ground truth: {gt_all[i]}, Prediction: {argmax_all[i]}')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# This visualizes one batch of images, but we want to visualize all images in the dataset.\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     plt.imshow(img_list[i].cpu().permute(1,2,0))\n",
    "#     plt.title(f'Ground truth: {gt_all[i]}, Prediction: {argmax_all[i]}')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for df_combined, we plot a batch of images listing the ground truth and confidence scores for each images using the 'filenames' column:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_batch(img_list, df_combined, start_idx=0):\n",
    "    # Create 4x3 grid with adjusted figure size\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Display up to 12 images per batch\n",
    "    for i in range(12):\n",
    "        idx = start_idx + i\n",
    "        if idx >= len(img_list):\n",
    "            break\n",
    "            \n",
    "        # Get data from dataframe\n",
    "        gt = df_combined.iloc[idx]['ground_truth']\n",
    "        pred = df_combined.iloc[idx]['predict_class']\n",
    "        conf = df_combined.iloc[idx]['confs']\n",
    "        fname = df_combined.iloc[idx]['filenames']\n",
    "            \n",
    "        # Display image and labels\n",
    "        axes[i].imshow(img_list[idx].cpu().permute(1,2,0))\n",
    "        axes[i].set_title(f'File: {fname}\\nGT: {gt}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "### NOTE: This function is designed to display a batch of 12 images at a time. ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(img_list, df_combined, num_batches=1, batch_size=12, grid_layout=(4,3)):\n",
    "    \"\"\"\n",
    "    Display specified number of image batches with metadata\n",
    "    \"\"\"\n",
    "    for batch in range(num_batches):\n",
    "        # Create figure and axes grid\n",
    "        rows, cols = grid_layout\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        # Calculate start index for current batch\n",
    "        start_idx = batch * batch_size\n",
    "        \n",
    "        # Display images in current batch\n",
    "        for i in range(batch_size):\n",
    "            idx = start_idx + i\n",
    "            if idx >= len(img_list):\n",
    "                break\n",
    "                \n",
    "            # Get metadata from dataframe\n",
    "            gt = df_combined.iloc[idx]['ground_truth']\n",
    "            pred = df_combined.iloc[idx]['predict_class']\n",
    "            conf = df_combined.iloc[idx]['confs']\n",
    "            fname = df_combined.iloc[idx][1]#'filenames']\n",
    "                \n",
    "            # Display image and labels\n",
    "            axes[i].imshow(img_list[idx].cpu().permute(1,2,0))\n",
    "            axes[i].set_title(f'File: {fname}\\nGT: {gt}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Clear unused subplots\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage:\n",
    "# Show first batch only\n",
    "display_images(img_list, df_combined)\n",
    "\n",
    "# Show first 3 batches\n",
    "# display_images(img_list, df_combined, num_batches=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 23  11:00am - this code will take the lowest confidence correct scores, and highest confidence incorrect scores\n",
    "\n",
    "def display_confidence_cases(img_list, df_combined, n_images=20, grid_layout=(5,4)):\n",
    "    \"\"\"Display lowest confidence correct class 1 and highest confidence incorrect class 1\"\"\"\n",
    "    \n",
    "    # Filter case 1: Correct class 1 predictions (lowest confidence)\n",
    "    correct_class1 = df_combined[\n",
    "        (df_combined['ground_truth'] == 1) & \n",
    "        (df_combined['predict_class'] == 1)\n",
    "    ].sort_values('confs', ascending=True).head(n_images)\n",
    "    \n",
    "    # Filter case 2: Incorrect predictions of class 1 (highest confidence)\n",
    "    incorrect_class1 = df_combined[\n",
    "        (df_combined['ground_truth'] == 1) & \n",
    "        (df_combined['predict_class'] == 0)\n",
    "    ].sort_values('confs', ascending=False).head(n_images)\n",
    "    \n",
    "    # Display both cases\n",
    "    for case, title in zip([correct_class1, incorrect_class1], \n",
    "                          ['Lowest Confidence Correct Class 1', \n",
    "                           'Highest Confidence Incorrect Class 1']):\n",
    "        \n",
    "        if len(case) == 0:\n",
    "            print(f\"No images found for {title}\")\n",
    "            continue\n",
    "            \n",
    "        rows, cols = grid_layout\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        n_display = min(len(case), len(axes))\n",
    "        \n",
    "        # Display available images\n",
    "        for i in range(n_display):\n",
    "            idx = case.index[i]\n",
    "            row = case.iloc[i]\n",
    "            \n",
    "            # Get image and metadata\n",
    "            gt = row['ground_truth']\n",
    "            pred = row['predict_class']\n",
    "            conf = row['confs']\n",
    "            fname = row[1]  # 'filenames'\n",
    "            \n",
    "            # Display image\n",
    "            axes[i].imshow(img_list[idx].cpu().permute(1,2,0))\n",
    "            axes[i].set_title(f'File: {fname}\\nGT: {gt}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Clear remaining subplots\n",
    "        for j in range(n_display, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage\n",
    "display_confidence_cases(img_list, df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confidence_cases(img_list, df_combined, n_images=20, grid_layout=(5,4)):\n",
    "    \"\"\"Display lowest confidence correct class 1 and highest confidence incorrect class 1\"\"\"\n",
    "    \n",
    "    # Filter cases\n",
    "    correct_class1 = df_combined[\n",
    "        (df_combined['ground_truth'] == 1) & \n",
    "        (df_combined['predict_class'] == 1)\n",
    "    ].sort_values('confs', ascending=True).head(n_images)\n",
    "    \n",
    "    incorrect_class1 = df_combined[\n",
    "        (df_combined['ground_truth'] == 1) & \n",
    "        (df_combined['predict_class'] == 0)\n",
    "    ].sort_values('confs', ascending=False).head(n_images)\n",
    "    \n",
    "    # Display both cases\n",
    "    for case, title in zip([correct_class1, incorrect_class1], \n",
    "                          ['Lowest Confidence Correct Class 1', \n",
    "                           'Highest Confidence Incorrect Class 1']):\n",
    "        \n",
    "        if len(case) == 0:\n",
    "            print(f\"No images found for {title}\")\n",
    "            continue\n",
    "            \n",
    "        rows, cols = grid_layout\n",
    "        # Increase figure height to accommodate title\n",
    "        fig = plt.figure(figsize=(cols*4, rows*4 + 1))\n",
    "        \n",
    "        # Add padding for title\n",
    "        plt.subplots_adjust(top=0.93)\n",
    "        \n",
    "        # Create subplot grid\n",
    "        gs = fig.add_gridspec(rows, cols)\n",
    "        axes = [fig.add_subplot(gs[i, j]) for i in range(rows) for j in range(cols)]\n",
    "        \n",
    "        n_display = min(len(case), len(axes))\n",
    "        \n",
    "        # Display available images\n",
    "        for i in range(n_display):\n",
    "            idx = case.index[i]\n",
    "            row = case.iloc[i]\n",
    "            \n",
    "            gt = row['ground_truth']\n",
    "            pred = row['predict_class']\n",
    "            conf = row['confs']\n",
    "            fname = row[1]\n",
    "            \n",
    "            axes[i].imshow(img_list[idx].cpu().permute(1,2,0))\n",
    "            axes[i].set_title(f'File: {fname}\\nGT: {gt}, Pred: {pred}\\nConf: {conf:.3f}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Clear remaining subplots\n",
    "        for j in range(n_display, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16, y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage\n",
    "display_confidence_cases(img_list, df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv4ecology2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
